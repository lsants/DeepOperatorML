# ------------------- Model architecture ------------------
PRECISION: float32
DEVICE: cpu
SEED: 42

TRAINING_STRATEGY: two_step
OUTPUT_HANDLING: split_outputs
BASIS_FUNCTIONS: 30
BRANCH_ARCHITECTURE: resnet
BRANCH_ACTIVATION: silu
BRANCH_DEGREE: 8
BRANCH_HIDDEN_LAYERS:
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 

TRUNK_ARCHITECTURE: resnet
TRUNK_ACTIVATION: silu
TRUNK_DEGREE: 8
TRUNK_HIDDEN_LAYERS:
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 

LOSS_FUNCTION: mse
INPUT_NORMALIZATION: 'minmax_0_1'  # Options: 'minmax_0_1', 'minmax_-1_1', 'standardize', 'none'
OUTPUT_NORMALIZATION: 'none'
TRUNK_FEATURE_EXPANSION: 20
RESCALING: none # p = number of basis functions. Can be none, 1/p or 1/sqrt(p)

# ------------------- Parameters for vanilla training ------------------
LEARNING_RATE: 0.001
EPOCHS: 10
BRANCH_BATCH_SIZE: 50
TRUNK_BATCH_SIZE: 50
ERROR_NORM: 2
STANDARD_PROGRESS_BAR_COLOR: 'green'
GLOBAL_OPTIMIZER_SCHEDULE:
  - EPOCHS: 5000
    OPTIMIZER: adam
    LEARNING_RATE: 0.0001
    L2_REGULARIZATION: 0.0001
    LR_SCHEDULER:
      STEP_SIZE: 1000
      GAMMA: 0.7

# ------------------- Parameters for POD training ----------------------
VAR_SHARE: 0.99999

# ------------------- Parameters for two step training ------------------
TRUNK_TRAIN_EPOCHS: 150
BRANCH_TRAIN_EPOCHS: 500

TRUNK_DECOMPOSITION: svd

PHASE_OPTIMIZER_SCHEDULE:
  trunk:
    - EPOCHS: 10000
      OPTIMIZER: adam
      LEARNING_RATE: 0.001
      L2_REGULARIZATION: 0.00001
    - EPOCHS: 30000
      OPTIMIZER: adam
      LEARNING_RATE: 0.001
      L2_REGULARIZATION: 0.00001
      LR_SCHEDULER:
        STEP_SIZE: 5000
        GAMMA: 0.7
    - EPOCHS: 110000
      OPTIMIZER: adam
      LEARNING_RATE: 0.0005
      L2_REGULARIZATION: 0.00001
  branch:
    - EPOCHS: 50000
      OPTIMIZER: adam
      LEARNING_RATE: 0.001
      L2_REGULARIZATION: 0.00001
      LR_SCHEDULER:
        STEP_SIZE: 5000
        GAMMA: 0.7
    - EPOCHS: 50000
      OPTIMIZER: adam
      LEARNING_RATE: 0.0001
      L2_REGULARIZATION: 0.00001
      LR_SCHEDULER:
        STEP_SIZE: 10000
        GAMMA: 0.5
    - EPOCHS: 50000
      OPTIMIZER: adam
      LEARNING_RATE: 0.0001
      L2_REGULARIZATION: 0.00001
      LR_SCHEDULER:
        STEP_SIZE: 10000
        GAMMA: 0.5