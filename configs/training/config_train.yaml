# ------------------- Model architecture ------------------
seed: 42

training_strategy: pod
output_handling: split_outputs
basis_functions: 30
branch_architecture: resnet
branch_activation: silu
branch_degree: 8
branch_hidden_layers:
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 

trunk_architecture: resnet
trunk_activation: silu
trunk_degree: 8
trunk_hidden_layers:
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 
  - 100 

loss_function: mse
transforms:
  branch:
    normalization: null
    feature_expansion: null
  trunk:
    normalization: 0_1_min_max  # 0_1_min_max, -1_1_min_max, standardize, null
    feature_expansion:
      type: cosine
      size: 10
  output_normalization: null
rescaling: 
  type: null
  exponent: -1 #  Used to rescale the output of the DeepONet by the number of basis functions to the power of the exponent (use exponent < 0).

# ------------------- Parameters for vanilla training ------------------
learning_rate: 0.001
epochs: 10
branch_batch_size: 50
trunk_batch_size: null
error: vector_l2
standard_progress_bar_color: 'green'
global_optimizer_schedule:
  - epochs: 5000
    optimizer: adam
    learning_rate: 0.0001
    l2_regularization: 0.0001
    lr_scheduler:
      step_size: 1000
      gamma: 0.7

# ------------------- Parameters for POD training ----------------------
var_share: 0.99999

# ------------------- Parameters for two step training ------------------
trunk_epochs: 150
branch_epochs: 500

trunk_decomposition: svd

phase_optimizer_schedule:
  trunk:
    - epochs: 10000
      optimizer: adam
      learning_rate: 0.001
      l2_regularization: 0.00001
    - epochs: 30000
      optimizer: adam
      learning_rate: 0.001
      l2_regularization: 0.00001
      lr_scheduler:
        step_size: 5000
        gamma: 0.7
    - epochs: 110000
      optimizer: adam
      learning_rate: 0.0005
      l2_regularization: 0.00001
  branch:
    - epochs: 50000
      optimizer: adam
      learning_rate: 0.001
      l2_regularization: 0.00001
      lr_scheduleR:
        step_size: 5000
        gamma: 0.7
    - epochs: 50000
      optimizer: adam
      learning_rate: 0.0001
      l2_regularization: 0.00001
      lr_scheduler:
        step_size: 10000
        gamma: 0.5
    - epochs: 50000
      optimizer: adam
      learning_rate: 0.0001
      l2_regularization: 0.00001
      lr_scheduler:
        step_size: 10000
        gamma: 0.5